{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fcn_model\n",
    "import fcn_dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dsa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define the model\n",
    "num_classes = 32\n",
    "model = fcn_model.FCN8s(num_classes).to(device)\n",
    "\n",
    "# Define the dataset and dataloader\n",
    "images_dir_train = \"train/\"\n",
    "labels_dir_train = \"train_labels/\"\n",
    "class_dict_path = \"class_dict.csv\"\n",
    "resolution = (384, 512)\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "camvid_dataset_train = fcn_dataset.CamVidDataset(root='CamVid/', images_dir=images_dir_train, labels_dir=labels_dir_train, class_dict_path=class_dict_path, resolution=resolution, crop=True)\n",
    "dataloader_train = torch.utils.data.DataLoader(camvid_dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "images_dir_val = \"val/\"\n",
    "labels_dir_val = \"val_labels/\"\n",
    "camvid_dataset_val = fcn_dataset.CamVidDataset(root='CamVid/', images_dir=images_dir_val, labels_dir=labels_dir_val, class_dict_path=class_dict_path, resolution=resolution, crop=False)\n",
    "dataloader_val = torch.utils.data.DataLoader(camvid_dataset_val, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "images_dir_test = \"test/\"\n",
    "labels_dir_test = \"test_labels/\"\n",
    "camvid_dataset_test = fcn_dataset.CamVidDataset(root='CamVid/', images_dir=images_dir_test, labels_dir=labels_dir_test, class_dict_path=class_dict_path, resolution=resolution, crop=False)\n",
    "dataloader_test = torch.utils.data.DataLoader(camvid_dataset_test, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function and optimizer\n",
    "def loss_fn(outputs, labels):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, device, save_pred=False):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    total_intersection = np.zeros(num_classes)\n",
    "    total_union = np.zeros(num_classes)\n",
    "    total_pixels_per_class = np.zeros(num_classes)\n",
    "\n",
    "\n",
    "    if save_pred:\n",
    "        pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "            total_pixels += labels.nelement()\n",
    "            total_correct_pixels += (predicted == labels).sum().item()\n",
    "            for cls in range(num_classes):\n",
    "                pred_inds = (predicted == cls)\n",
    "                target_inds = (labels == cls)\n",
    "                intersection = (pred_inds[target_inds]).sum().item()\n",
    "                total_intersection[cls] += intersection\n",
    "                total_union[cls] += pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "\n",
    "                # Update pixels per class for frequency weighted IoU\n",
    "                total_pixels_per_class[cls] += target_inds.sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if save_pred:\n",
    "                pred_list.append(predicted.cpu().numpy())\n",
    "        \n",
    "                \n",
    "\n",
    "        # pixel_acc = ...\n",
    "        # mean_iou = ...\n",
    "        # freq_iou = ...\n",
    "        pixel_accuracy = total_correct_pixels / total_pixels\n",
    "        mean_accuracy = np.mean(total_intersection / (total_pixels_per_class + 1e-10))\n",
    "        mean_iou = np.mean(total_intersection / (total_union + 1e-10))\n",
    "        freq_iou = (total_pixels_per_class / total_pixels).dot(total_intersection / (total_union + 1e-10))\n",
    "\n",
    "        \n",
    "        loss = sum(loss_list) / len(loss_list)\n",
    "        # print('Loss: {:.4f}'.format(loss))\n",
    "        print('Pixel accuracy: {:.4f}, Mean IoU: {:.4f}, Frequency weighted IoU: {:.4f}, Loss: {:.4f}'.format(pixel_accuracy, mean_iou, freq_iou, loss))\n",
    "\n",
    "    if save_pred:\n",
    "        pred_list = np.concatenate(pred_list, axis=0)\n",
    "        np.save('test_pred.npy', pred_list)\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, dataloader, device):\n",
    "    log_dir = \"vis/\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    cls_dict = dataloader.dataset.class_dict.copy()\n",
    "    cls_list = [cls_dict[i] for i in range(len(cls_dict))]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ind, (images, labels) in enumerate(tqdm(dataloader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            images_vis = fcn_dataset.rev_normalize(images)\n",
    "            # Save the images and labels\n",
    "            img = images_vis[0].permute(1, 2, 0).cpu().numpy()\n",
    "            img = img * 255\n",
    "            img = img.astype('uint8')\n",
    "            label = labels[0].cpu().numpy()\n",
    "            pred = predicted[0].cpu().numpy()\n",
    "\n",
    "            label_img = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "            pred_img = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "            for j in range(len(cls_list)):\n",
    "                mask = label == j\n",
    "                label_img[mask] = cls_list[j][0]\n",
    "                mask = pred == j\n",
    "                pred_img[mask] = cls_list[j][0]\n",
    "            # horizontally concatenate the image, label, and prediction, and save the visualization\n",
    "            vis_img = np.concatenate([img, label_img, pred_img], axis=1)\n",
    "            vis_img = Image.fromarray(vis_img)\n",
    "            vis_img.save(os.path.join(log_dir, 'img_{:04d}.png'.format(ind)))\n",
    "            \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/24], Loss: 3.8252\n",
      "Epoch [1/50], Step [20/24], Loss: 1.9497\n",
      "Pixel accuracy: 0.5979, Mean IoU: 0.0651, Frequency weighted IoU: 0.4131, Loss: 1.3934\n",
      "Epoch [2/50], Step [10/24], Loss: 1.3182\n",
      "Epoch [2/50], Step [20/24], Loss: 1.0135\n",
      "Pixel accuracy: 0.6936, Mean IoU: 0.0894, Frequency weighted IoU: 0.5222, Loss: 1.1111\n",
      "Epoch [3/50], Step [10/24], Loss: 0.9473\n",
      "Epoch [3/50], Step [20/24], Loss: 0.8263\n",
      "Pixel accuracy: 0.7589, Mean IoU: 0.1232, Frequency weighted IoU: 0.6211, Loss: 0.8727\n",
      "Epoch [4/50], Step [10/24], Loss: 0.7645\n",
      "Epoch [4/50], Step [20/24], Loss: 0.7605\n",
      "Pixel accuracy: 0.7804, Mean IoU: 0.1386, Frequency weighted IoU: 0.6460, Loss: 0.8069\n",
      "Epoch [5/50], Step [10/24], Loss: 0.7542\n",
      "Epoch [5/50], Step [20/24], Loss: 0.6455\n",
      "Pixel accuracy: 0.7717, Mean IoU: 0.1562, Frequency weighted IoU: 0.6501, Loss: 0.8954\n",
      "Epoch [6/50], Step [10/24], Loss: 0.8988\n",
      "Epoch [6/50], Step [20/24], Loss: 0.7309\n",
      "Pixel accuracy: 0.7952, Mean IoU: 0.1539, Frequency weighted IoU: 0.6674, Loss: 0.7465\n",
      "Epoch [7/50], Step [10/24], Loss: 0.6976\n",
      "Epoch [7/50], Step [20/24], Loss: 0.6218\n",
      "Pixel accuracy: 0.8185, Mean IoU: 0.1791, Frequency weighted IoU: 0.6951, Loss: 0.6464\n",
      "Epoch [8/50], Step [10/24], Loss: 0.6192\n",
      "Epoch [8/50], Step [20/24], Loss: 0.6067\n",
      "Pixel accuracy: 0.8260, Mean IoU: 0.1900, Frequency weighted IoU: 0.7082, Loss: 0.6263\n",
      "Epoch [9/50], Step [10/24], Loss: 0.5919\n",
      "Epoch [9/50], Step [20/24], Loss: 0.5751\n",
      "Pixel accuracy: 0.8261, Mean IoU: 0.1991, Frequency weighted IoU: 0.7173, Loss: 0.6172\n",
      "Epoch [10/50], Step [10/24], Loss: 0.5387\n",
      "Epoch [10/50], Step [20/24], Loss: 0.4995\n",
      "Pixel accuracy: 0.8379, Mean IoU: 0.2224, Frequency weighted IoU: 0.7309, Loss: 0.5649\n",
      "Epoch [11/50], Step [10/24], Loss: 0.4672\n",
      "Epoch [11/50], Step [20/24], Loss: 0.4804\n",
      "Pixel accuracy: 0.8173, Mean IoU: 0.2194, Frequency weighted IoU: 0.7171, Loss: 0.6692\n",
      "Epoch [12/50], Step [10/24], Loss: 0.5739\n",
      "Epoch [12/50], Step [20/24], Loss: 0.5586\n",
      "Pixel accuracy: 0.8370, Mean IoU: 0.2214, Frequency weighted IoU: 0.7223, Loss: 0.5835\n",
      "Epoch [13/50], Step [10/24], Loss: 0.4767\n",
      "Epoch [13/50], Step [20/24], Loss: 0.4818\n",
      "Pixel accuracy: 0.8492, Mean IoU: 0.2433, Frequency weighted IoU: 0.7448, Loss: 0.5282\n",
      "Epoch [14/50], Step [10/24], Loss: 0.4613\n",
      "Epoch [14/50], Step [20/24], Loss: 0.4709\n",
      "Pixel accuracy: 0.8479, Mean IoU: 0.2489, Frequency weighted IoU: 0.7502, Loss: 0.5259\n",
      "Epoch [15/50], Step [10/24], Loss: 0.4273\n",
      "Epoch [15/50], Step [20/24], Loss: 0.4684\n",
      "Pixel accuracy: 0.8533, Mean IoU: 0.2648, Frequency weighted IoU: 0.7520, Loss: 0.5071\n",
      "Epoch [16/50], Step [10/24], Loss: 0.4731\n",
      "Epoch [16/50], Step [20/24], Loss: 0.4556\n",
      "Pixel accuracy: 0.8510, Mean IoU: 0.2553, Frequency weighted IoU: 0.7560, Loss: 0.5150\n",
      "Epoch [17/50], Step [10/24], Loss: 0.4418\n",
      "Epoch [17/50], Step [20/24], Loss: 0.4419\n",
      "Pixel accuracy: 0.8598, Mean IoU: 0.2647, Frequency weighted IoU: 0.7643, Loss: 0.5007\n",
      "Epoch [18/50], Step [10/24], Loss: 0.4168\n",
      "Epoch [18/50], Step [20/24], Loss: 0.3899\n",
      "Pixel accuracy: 0.8646, Mean IoU: 0.2879, Frequency weighted IoU: 0.7699, Loss: 0.4586\n",
      "Epoch [19/50], Step [10/24], Loss: 0.3803\n",
      "Epoch [19/50], Step [20/24], Loss: 0.3695\n",
      "Pixel accuracy: 0.8677, Mean IoU: 0.3011, Frequency weighted IoU: 0.7790, Loss: 0.4507\n",
      "Epoch [20/50], Step [10/24], Loss: 0.3698\n",
      "Epoch [20/50], Step [20/24], Loss: 0.3623\n",
      "Pixel accuracy: 0.8635, Mean IoU: 0.2969, Frequency weighted IoU: 0.7766, Loss: 0.4625\n",
      "Epoch [21/50], Step [10/24], Loss: 0.3851\n",
      "Epoch [21/50], Step [20/24], Loss: 0.3340\n",
      "Pixel accuracy: 0.8498, Mean IoU: 0.2913, Frequency weighted IoU: 0.7593, Loss: 0.5104\n",
      "Epoch [22/50], Step [10/24], Loss: 0.3615\n",
      "Epoch [22/50], Step [20/24], Loss: 0.3854\n",
      "Pixel accuracy: 0.8714, Mean IoU: 0.3259, Frequency weighted IoU: 0.7846, Loss: 0.4327\n",
      "Epoch [23/50], Step [10/24], Loss: 0.3502\n",
      "Epoch [23/50], Step [20/24], Loss: 0.4824\n",
      "Pixel accuracy: 0.8362, Mean IoU: 0.2614, Frequency weighted IoU: 0.7281, Loss: 0.6016\n",
      "Epoch [24/50], Step [10/24], Loss: 0.4685\n",
      "Epoch [24/50], Step [20/24], Loss: 0.4277\n",
      "Pixel accuracy: 0.8613, Mean IoU: 0.3057, Frequency weighted IoU: 0.7711, Loss: 0.4665\n",
      "Epoch [25/50], Step [10/24], Loss: 0.4129\n",
      "Epoch [25/50], Step [20/24], Loss: 0.3695\n",
      "Pixel accuracy: 0.8648, Mean IoU: 0.3024, Frequency weighted IoU: 0.7785, Loss: 0.4602\n",
      "Epoch [26/50], Step [10/24], Loss: 0.3495\n",
      "Epoch [26/50], Step [20/24], Loss: 0.3675\n",
      "Pixel accuracy: 0.8707, Mean IoU: 0.3180, Frequency weighted IoU: 0.7843, Loss: 0.4331\n",
      "Epoch [27/50], Step [10/24], Loss: 0.3292\n",
      "Epoch [27/50], Step [20/24], Loss: 0.3359\n",
      "Pixel accuracy: 0.8677, Mean IoU: 0.3180, Frequency weighted IoU: 0.7793, Loss: 0.4503\n",
      "Epoch [28/50], Step [10/24], Loss: 0.3388\n",
      "Epoch [28/50], Step [20/24], Loss: 0.3086\n",
      "Pixel accuracy: 0.8774, Mean IoU: 0.3337, Frequency weighted IoU: 0.7936, Loss: 0.4212\n",
      "Epoch [29/50], Step [10/24], Loss: 0.3096\n",
      "Epoch [29/50], Step [20/24], Loss: 0.3106\n",
      "Pixel accuracy: 0.8785, Mean IoU: 0.3418, Frequency weighted IoU: 0.7940, Loss: 0.4151\n",
      "Epoch [30/50], Step [10/24], Loss: 0.3364\n",
      "Epoch [30/50], Step [20/24], Loss: 0.3055\n",
      "Pixel accuracy: 0.8791, Mean IoU: 0.3475, Frequency weighted IoU: 0.7964, Loss: 0.3970\n",
      "Epoch [31/50], Step [10/24], Loss: 0.3132\n",
      "Epoch [31/50], Step [20/24], Loss: 0.2899\n",
      "Pixel accuracy: 0.8791, Mean IoU: 0.3372, Frequency weighted IoU: 0.7969, Loss: 0.4011\n",
      "Epoch [32/50], Step [10/24], Loss: 0.3009\n",
      "Epoch [32/50], Step [20/24], Loss: 0.3199\n",
      "Pixel accuracy: 0.8700, Mean IoU: 0.3192, Frequency weighted IoU: 0.7864, Loss: 0.4377\n",
      "Epoch [33/50], Step [10/24], Loss: 0.3304\n",
      "Epoch [33/50], Step [20/24], Loss: 0.3058\n",
      "Pixel accuracy: 0.8825, Mean IoU: 0.3649, Frequency weighted IoU: 0.8008, Loss: 0.3984\n",
      "Epoch [34/50], Step [10/24], Loss: 0.3078\n",
      "Epoch [34/50], Step [20/24], Loss: 0.2810\n",
      "Pixel accuracy: 0.8812, Mean IoU: 0.3573, Frequency weighted IoU: 0.8000, Loss: 0.4023\n",
      "Epoch [35/50], Step [10/24], Loss: 0.2788\n",
      "Epoch [35/50], Step [20/24], Loss: 0.2774\n",
      "Pixel accuracy: 0.8857, Mean IoU: 0.3758, Frequency weighted IoU: 0.8053, Loss: 0.3884\n",
      "Epoch [36/50], Step [10/24], Loss: 0.2780\n",
      "Epoch [36/50], Step [20/24], Loss: 0.2670\n",
      "Pixel accuracy: 0.8841, Mean IoU: 0.3803, Frequency weighted IoU: 0.8057, Loss: 0.3895\n",
      "Epoch [37/50], Step [10/24], Loss: 0.2591\n",
      "Epoch [37/50], Step [20/24], Loss: 0.2752\n",
      "Pixel accuracy: 0.8829, Mean IoU: 0.3732, Frequency weighted IoU: 0.8058, Loss: 0.3984\n",
      "Epoch [38/50], Step [10/24], Loss: 0.2747\n",
      "Epoch [38/50], Step [20/24], Loss: 0.2787\n",
      "Pixel accuracy: 0.8896, Mean IoU: 0.3929, Frequency weighted IoU: 0.8116, Loss: 0.3774\n",
      "Epoch [39/50], Step [10/24], Loss: 0.2614\n",
      "Epoch [39/50], Step [20/24], Loss: 0.2477\n",
      "Pixel accuracy: 0.8866, Mean IoU: 0.3789, Frequency weighted IoU: 0.8067, Loss: 0.3861\n",
      "Epoch [40/50], Step [10/24], Loss: 0.2597\n",
      "Epoch [40/50], Step [20/24], Loss: 0.2507\n",
      "Pixel accuracy: 0.8893, Mean IoU: 0.4008, Frequency weighted IoU: 0.8099, Loss: 0.3726\n",
      "Epoch [41/50], Step [10/24], Loss: 0.2387\n",
      "Epoch [41/50], Step [20/24], Loss: 0.2410\n",
      "Pixel accuracy: 0.8867, Mean IoU: 0.3865, Frequency weighted IoU: 0.8086, Loss: 0.4002\n",
      "Epoch [42/50], Step [10/24], Loss: 0.2747\n",
      "Epoch [42/50], Step [20/24], Loss: 0.3041\n",
      "Pixel accuracy: 0.8736, Mean IoU: 0.3478, Frequency weighted IoU: 0.7882, Loss: 0.4709\n",
      "Epoch [43/50], Step [10/24], Loss: 0.2998\n",
      "Epoch [43/50], Step [20/24], Loss: 0.2871\n",
      "Pixel accuracy: 0.8841, Mean IoU: 0.3675, Frequency weighted IoU: 0.8038, Loss: 0.4061\n",
      "Epoch [44/50], Step [10/24], Loss: 0.2529\n",
      "Epoch [44/50], Step [20/24], Loss: 0.2853\n",
      "Pixel accuracy: 0.8904, Mean IoU: 0.3976, Frequency weighted IoU: 0.8144, Loss: 0.3684\n",
      "Epoch [45/50], Step [10/24], Loss: 0.2449\n",
      "Epoch [45/50], Step [20/24], Loss: 0.2449\n",
      "Pixel accuracy: 0.8935, Mean IoU: 0.4105, Frequency weighted IoU: 0.8160, Loss: 0.3597\n",
      "Epoch [46/50], Step [10/24], Loss: 0.2360\n",
      "Epoch [46/50], Step [20/24], Loss: 0.2322\n",
      "Pixel accuracy: 0.8913, Mean IoU: 0.4064, Frequency weighted IoU: 0.8116, Loss: 0.3683\n",
      "Epoch [47/50], Step [10/24], Loss: 0.2227\n",
      "Epoch [47/50], Step [20/24], Loss: 0.2302\n",
      "Pixel accuracy: 0.8933, Mean IoU: 0.4222, Frequency weighted IoU: 0.8199, Loss: 0.3616\n",
      "Epoch [48/50], Step [10/24], Loss: 0.2205\n",
      "Epoch [48/50], Step [20/24], Loss: 0.2205\n",
      "Pixel accuracy: 0.8961, Mean IoU: 0.4167, Frequency weighted IoU: 0.8222, Loss: 0.3601\n",
      "Epoch [49/50], Step [10/24], Loss: 0.2256\n",
      "Epoch [49/50], Step [20/24], Loss: 0.2170\n",
      "Pixel accuracy: 0.8978, Mean IoU: 0.4180, Frequency weighted IoU: 0.8222, Loss: 0.3712\n",
      "Epoch [50/50], Step [10/24], Loss: 0.2059\n",
      "Epoch [50/50], Step [20/24], Loss: 0.2226\n",
      "Pixel accuracy: 0.8957, Mean IoU: 0.4114, Frequency weighted IoU: 0.8231, Loss: 0.3528\n",
      "====================\n",
      "Finished Training, evaluating the model on the test set\n",
      "Pixel accuracy: 0.8648, Mean IoU: 0.3513, Frequency weighted IoU: 0.7782, Loss: 0.4698\n",
      "====================\n",
      "Visualizing the model on the test set, the results will be saved in the vis/ directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:32<00:00,  7.17it/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Train the model\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(dataloader_train):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(dataloader_train), sum(loss_list)/len(loss_list)))\n",
    "            loss_list = []\n",
    "\n",
    "    # eval the model        \n",
    "    eval_model(model, dataloader_val, device)\n",
    "\n",
    "print('='*20)\n",
    "print('Finished Training, evaluating the model on the test set')\n",
    "eval_model(model, dataloader_test, device, save_pred=True)\n",
    "\n",
    "print('='*20)\n",
    "print('Visualizing the model on the test set, the results will be saved in the vis/ directory')\n",
    "visualize_model(model, dataloader_test, device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f041abc9417f01065ea833f8916bc87cc743515d4c75dfcfa1218d6956e4adb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
